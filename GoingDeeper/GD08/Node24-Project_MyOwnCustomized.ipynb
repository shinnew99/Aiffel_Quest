{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c0149e",
   "metadata": {},
   "source": [
    "|평가문항|상세기준|\n",
    "|:------|:---|\n",
    "|1. 모델과 데이터를 정상적으로 불러오고, 작동하는 것을 확인하였다. |klue/bert-base를 NSMC 데이터셋으로 fine-tuning하여, 모델이 정상적으로 작동하는 것을 확인하였다.|\n",
    "|2. Preprocessing을 개선하고, fine-tuning을 통해 모델의 성능을 개선시켰다. | Validation accuracy를 90%이상으로 개선하였다.|\n",
    "|3. 모델학습에 Bucketing을 성공적으로 적용하고, 그 결과를 비교분석하였다. | Bucketing Task를 수행하여 fine-tuning 시 연산 속도와 모델 성능가느이 trade-off 관계가 발생하는지 여부를 확인하고, 분서갛ㄴ 결과를 제시하였다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a256b2",
   "metadata": {},
   "source": [
    "### Step 0. 패키지 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f50376f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers==4.4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a50930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41871775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 15 04:00:27 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\r\n",
      "| N/A   58C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a07a8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27e78a1c",
   "metadata": {},
   "source": [
    "### Step 1. NSMC 데이터 분석 및 Huggingface dataset 구성\n",
    "\n",
    "데이터셋은 깃허브에서 다운받거나, Huggingface datasets에서 가져올 수 있습니다. 앞에서 배운 방법들을 활용해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7584d91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dbbf86c26f84344b9d4254da5dfb85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "huggingface_dataset = load_dataset('nsmc')\n",
    "print(huggingface_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66b4748f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'document', 'label']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = huggingface_dataset['train']\n",
    "cols = train.column_names\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aede385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 9976970\n",
      "document : 아 더빙.. 진짜 짜증나네요 목소리\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 3819312\n",
      "document : 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "label : 1\n",
      "\n",
      "\n",
      "id : 10265843\n",
      "document : 너무재밓었다그래서보는것을추천한다\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 9045019\n",
      "document : 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 6483659\n",
      "document : 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n",
      "label : 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for col in cols:\n",
    "        print(col, \":\", train[col][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72f7e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b1e540f",
   "metadata": {},
   "source": [
    "### Step 2. klue/bert-base model 및 tokenizer 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb5c1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet():\n",
    "    def __init__(self, dataset_name, model_name, padding='max_length'):\n",
    "        super(DataSet, self).__init__()\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        self.padding = padding\n",
    "        dataset = self._set(dataset_name)\n",
    "        \n",
    "        self.train = dataset['train']\n",
    "        self.test = dataset['test']\n",
    "        self.valid = dataset['valid']\n",
    "                                        \n",
    "            \n",
    "    def transform(self, data):\n",
    "        return self.tokenizer(\n",
    "            data['document'],\n",
    "            truncation=True,\n",
    "            padding=self.padding,\n",
    "#             padding='max_length',\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "       \n",
    "        \n",
    "    def _set(self, dataset_name):\n",
    "        data = load_dataset(dataset_name)\n",
    "        train_valid = data['train'].train_test_split(test_size=0.2)\n",
    "                \n",
    "        return DatasetDict({\n",
    "            'train': train_valid['train'],\n",
    "            'valid': train_valid['test'],\n",
    "            'test': data['test']\n",
    "        }).map(self.transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d1ae63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ba42f7256c4b4b9d4d094a9d880bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-a3c98fba040dae08.arrow and /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-4552e18e170dc7b2.arrow\n",
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-9957dc54dc1c0b75.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49cf821a27f64c0d8ca1ba36ba50637b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-cb59f68dd3aff018.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'nsmc'\n",
    "model_name = 'klue/bert-base'\n",
    "output_dir = './results'\n",
    "    \n",
    "dataset = DataSet(dataset_name, model_name, padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25467e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab7e8368",
   "metadata": {},
   "source": [
    "### Step 3. 위에서 불러온 tokenizer으로 데이터셋을 전처리하고, model 학습 진행해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bf8b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier():\n",
    "    def __init__(self, model_name, dataset, training_arguments):\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.trainer = self._set(training_arguments)\n",
    "    \n",
    "    \n",
    "    def compute_metrics(self, pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        return { 'accuracy': accuracy_score(labels, preds) }\n",
    "    \n",
    "    \n",
    "    def _set(self, training_arguments):\n",
    "        return Trainer(\n",
    "            model=self.model,           \n",
    "            args=training_arguments,           \n",
    "            train_dataset=self.dataset.train,\n",
    "            eval_dataset=self.dataset.valid,       \n",
    "            compute_metrics=self.compute_metrics,\n",
    "            data_collator=DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def fine_tuning(self):\n",
    "        return self.trainer.train()\n",
    "        \n",
    "    \n",
    "    def evaluate(self):\n",
    "        return self.trainer.evaluate(self.dataset.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93df701",
   "metadata": {},
   "source": [
    "#### model1: batch_size 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4a849f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 35:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.284400</td>\n",
       "      <td>0.291624</td>\n",
       "      <td>0.903533</td>\n",
       "      <td>107.014100</td>\n",
       "      <td>280.337000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1081: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1081: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1081: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1081: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1081: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1081: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1081: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15000, training_loss=0.3217869557698568, metrics={'train_runtime': 2116.6689, 'train_samples_per_second': 7.087, 'total_flos': 4100680893586464.0, 'epoch': 1.0, 'init_mem_cpu_alloc_delta': 1699512, 'init_mem_gpu_alloc_delta': 443266560, 'init_mem_cpu_peaked_delta': 18258, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 1092153, 'train_mem_gpu_alloc_delta': 1332549120, 'train_mem_cpu_peaked_delta': 103720035, 'train_mem_gpu_peaked_delta': 1037661696})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = Classifier(\n",
    "    model_name,\n",
    "    dataset,\n",
    "    TrainingArguments(\n",
    "        output_dir,\n",
    "        evaluation_strategy = 'epoch',\n",
    "        learning_rate = 2e-5,\n",
    "        per_device_train_batch_size = 16,\n",
    "        per_device_eval_batch_size = 16,\n",
    "        num_train_epochs = 1,\n",
    "        warmup_steps = 1000,\n",
    "        weight_decay = 0.01,\n",
    "        fp16 = True,\n",
    "    ))\n",
    "\n",
    "model1.fine_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c2cc392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 02:57]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.29693958163261414,\n",
       " 'eval_accuracy': 0.9013,\n",
       " 'eval_runtime': 177.0711,\n",
       " 'eval_samples_per_second': 282.372,\n",
       " 'epoch': 1.0,\n",
       " 'eval_mem_cpu_alloc_delta': 859349,\n",
       " 'eval_mem_gpu_alloc_delta': 0,\n",
       " 'eval_mem_cpu_peaked_delta': 3056413,\n",
       " 'eval_mem_gpu_peaked_delta': 34402304}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f564e08d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a237bc8",
   "metadata": {},
   "source": [
    "### Step 4. Fine-tuning을 통하여 모델 성능(accuracy)향상시키기\n",
    "\n",
    "- 데이터 전처리, TrainingArguments 등을 조정하여 모델의 정확도를 90% 이상으로 끌어올려봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6944c024",
   "metadata": {},
   "source": [
    "#### model2: accumulation_steps 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72f5606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 25:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.248700</td>\n",
       "      <td>0.249789</td>\n",
       "      <td>0.901733</td>\n",
       "      <td>110.500000</td>\n",
       "      <td>271.493000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1081: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1081: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1081: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=0.29384472147623697, metrics={'train_runtime': 1549.6869, 'train_samples_per_second': 4.84, 'total_flos': 4100680893586464.0, 'epoch': 1.0, 'init_mem_cpu_alloc_delta': 1793072, 'init_mem_gpu_alloc_delta': 443266560, 'init_mem_cpu_peaked_delta': 378242, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 505311, 'train_mem_gpu_alloc_delta': 1329796608, 'train_mem_cpu_peaked_delta': 103761879, 'train_mem_gpu_peaked_delta': 1058182144})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Classifier(\n",
    "    model_name,\n",
    "    dataset,\n",
    "    TrainingArguments(\n",
    "        output_dir, \n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,   \n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        gradient_accumulation_steps=2,\n",
    "        eval_accumulation_steps=2,\n",
    "        num_train_epochs=1,\n",
    "        warmup_steps=1000, \n",
    "        weight_decay=0.01,                 \n",
    "        fp16=True,\n",
    "    ))\n",
    "\n",
    "model2.fine_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1a6ccd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 03:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.25234317779541016,\n",
       " 'eval_accuracy': 0.9013,\n",
       " 'eval_runtime': 183.344,\n",
       " 'eval_samples_per_second': 272.711,\n",
       " 'epoch': 1.0,\n",
       " 'eval_mem_cpu_alloc_delta': 877795,\n",
       " 'eval_mem_gpu_alloc_delta': 0,\n",
       " 'eval_mem_cpu_peaked_delta': 3070476,\n",
       " 'eval_mem_gpu_peaked_delta': 34960384}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71684aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0c08242",
   "metadata": {},
   "source": [
    "### Step 5. Bucketing을 작용하여 학습시키고, Step 4의 결과와의 비교\n",
    "\n",
    "아래 링크를 바탕으로 bucketing과 dynamic padding이 무엇인지 알아보고, 이들을 적용하여 model을 학습시킵니다.\n",
    "\n",
    "- Data Collator\n",
    "- Trainer.TrainingArguments 의 group_by_length\n",
    "\n",
    "STEP 4에 학습한 결과와 bucketing을 적용하여 학습시킨 결과를 비교해보고, 모델 성능 향상과 훈련 시간 두 가지 측면에서 각각 어떤 이점이 있는지 비교해봅시다.\n",
    "\n",
    "#### model3: group_by_length 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "045507fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 35:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.286300</td>\n",
       "      <td>0.298082</td>\n",
       "      <td>0.902367</td>\n",
       "      <td>106.828800</td>\n",
       "      <td>280.823000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1081: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1081: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1081: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1081: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1081: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1081: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  torch.nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15000, training_loss=0.32392432454427084, metrics={'train_runtime': 2118.8026, 'train_samples_per_second': 7.079, 'total_flos': 4100680893586464.0, 'epoch': 1.0, 'init_mem_cpu_alloc_delta': 46909, 'init_mem_gpu_alloc_delta': 443266560, 'init_mem_cpu_peaked_delta': 18258, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 512960, 'train_mem_gpu_alloc_delta': 1329796608, 'train_mem_cpu_peaked_delta': 103765442, 'train_mem_gpu_peaked_delta': 1057494016})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Classifier(\n",
    "    model_name,\n",
    "    dataset,\n",
    "    TrainingArguments(\n",
    "        output_dir, \n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,   \n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "#         gradient_accumulation_steps=2,\n",
    "#         eval_accumulation_steps=2,\n",
    "        num_train_epochs=1,\n",
    "        warmup_steps=1000, \n",
    "        weight_decay=0.01,                 \n",
    "        fp16=True,\n",
    "#         group_by_length=True\n",
    "    ))\n",
    "\n",
    "model3.fine_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a770556e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 05:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2991721034049988,\n",
       " 'eval_accuracy': 0.90218,\n",
       " 'eval_runtime': 354.8857,\n",
       " 'eval_samples_per_second': 140.89,\n",
       " 'epoch': 1.0,\n",
       " 'eval_mem_cpu_alloc_delta': 859196,\n",
       " 'eval_mem_gpu_alloc_delta': 0,\n",
       " 'eval_mem_cpu_peaked_delta': 3054902,\n",
       " 'eval_mem_gpu_peaked_delta': 34402304}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d62d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78b73ac9",
   "metadata": {},
   "source": [
    "### 회고록\n",
    "\n",
    "- 우연히 첫번쨰 모델에서 정확도가 90%가 넘었다. \n",
    "- 따라서 두번째 모델에서 파라미터 튜닝을 시도해봐도 첫번쨰 모델과 크게 달라진건 없었다.\n",
    "- Bucketing task를 수행했을때와 fine-tuning 시 연산 속도와 모델성능간의 trade-off관계가 발생하긴 한다. 시간이 훨씬 많이 걸린다. 하지만 전반적이 성능 비슷하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9388ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
